dataset: "cho_img"

use_compile: False
enable_xformers_memory_efficient_attention: True
gradient_checkpointing: True
use_magic: False 
SophiaG: False
lr: 1e-5
weight_decay: 0

use_margin: True
margin: 0.01

use_clip_norm: True
clip_max_norm: 1.0
start_clip_epoch: 10

adjacency_weight: True

# space_encoder_ckpt: "pretrain_space_encoder/w19_cho_space_encoder.pt"
space_encoder_ckpt: "pretrain_space_encoder/cho_619.pt"

# model: "DiM-B/2"
# use_attention: False
use_covariance: True  # to compute cov
balance_alpha: 0.1
use_local_attention: True  # to use cross attention after mamba 


# use_wt: True
# wt_margin: 0.005

global_seed: 25126
accumulation_steps: 1   #or 10
log_every: 10
ckpt_every_encoder: 20_000

image_size: 128
ckpt_every: 5_000
num_workers: 16
dt_rank: 16 
d_state: 128
use_image_num: 8  
frame_interval: 3
num_frames: 17
frame_num_in_video: 25

use_wt: True
wt_margin: 0.01

results_dir: "results/cho"
frame_data_txt: "datasets/CholecTriplet_list.txt"
frame_data_path: "."
data_path: "datasets/CholecTriplet"

#load from pre-train
init_from_pretrain_ckpt: True
pretrain_ckpt_path: "results/cho/019-MedSora-B-2/checkpoints/1570000.pt"
init_train_steps: 1570_000


#sample
load_ckpt_type: "model"
num_sampling_steps: 100
test_data_path: "datasets/CholecTriplet"
sample_global_seed: 25127
sample_num_workers: 4
create_video_num: 2048  #for noise2video
sample_method: "ddim"  #ddim, ddpm
save_video_path: "./sample_results/cho_mamba23_ddim_model"


pretrain_alpha: 1.0
frame_K: 4
pretrain_lambda_param: 1


#CUDA_VISIBLE_DEVICES=1,2 TRITON_PTXAS_PATH=/opt/conda/envs/Di/bin/ptxas  torchrun --master_port=12045 --nnodes=1 --nproc_per_node=2 train.py --model DiM-XL/2 --epoch 300 --global-batch-size 6 --what2video N2V --config config/col/col_train.yaml --use-mamba2

#CUDA_VISIBLE_DEVICES=0 TRITON_PTXAS_PATH=/opt/conda/envs/DiffMaSora/bin/ptxas torchrun --master_port=12749 --nnodes=1 --nproc_per_node=1 sample.py --model DiM-XL/2  --sample-global-batch-size 1 --what2video N2V --config config/col/col_train.yaml --use-mamba2 --ckpt results/col/068-DiM-XL-2/checkpoints/0080000.pt

# VAE
vae_results_dir: "results/vae/cho"
vae_global_seed: 25126
vae_epochs: 300
vae_batch_size: 1
vae_test_results_dir: "results/vae/cho/test/"
vae_lr: 2.0e-7
vae_ffl_weight: 0.1
vae_lpips_weight: 0.1
#load from pre-train-vae
vae_init_from_pretrain_ckpt: False
vae_pretrain_ckpt_path:  "" 
vae_sample_batch_size: 1
vae_test_ckpt: ""


# flow model
flow_model_path: "flow_models/raft-things.pth"

# DINO model
# dino_model_type: "dinov2_vitb14"  #dinov2_vits14, dinov2_vitb14, dinov2_vitl14, dinov2_vitg14
dino_weights_path: "dino_model/dino_vitbase8_pretrain_full_checkpoint.pth"  #dino_vits16, dino_vits8, dino_vitb16, dino_vitb8, dino_resnet50


#CUDA_VISIBLE_DEVICES=0 TRITON_PTXAS_PATH=/opt/conda/envs/newSora/bin/ptxas torchrun --master_port=12475 --nnodes=1 --nproc_per_node=1 test_flow_2.py --model MedSora-B/2 --global-batch-size 1 --config config/cho/cho_train.yaml  --use-mamba2 --use-local-cov